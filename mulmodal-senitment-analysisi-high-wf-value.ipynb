{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":46125,"databundleVersionId":4972941,"sourceType":"competition"}],"dockerImageVersionId":30397,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align:center;font-weight: 900; font-size:40px;\"> Multimodal Sentiment Analysis Higher Accuracy </p>","metadata":{}},{"cell_type":"markdown","source":"**More robust vgg19 and xlm-roberta******","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=5,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=3,\n            min_lr=1e-6\n        )\n    ]\n\n    # Data augmentation for images\n    train_images_augmented = augment_images(train_images)\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:24:30.074829Z","iopub.execute_input":"2025-01-06T18:24:30.075204Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [00:51<00:30, 38.45it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:16<00:00, 45.73it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 45.40it/s]\nAll model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n186/186 [==============================] - 153s 737ms/step - loss: 0.9669 - accuracy: 0.5488 - val_loss: 0.9261 - val_accuracy: 0.6305 - lr: 1.0000e-04\nEpoch 2/20\n186/186 [==============================] - 130s 700ms/step - loss: 0.9107 - accuracy: 0.5808 - val_loss: 0.8410 - val_accuracy: 0.6305 - lr: 1.0000e-04\nEpoch 3/20\n186/186 [==============================] - 130s 699ms/step - loss: 0.8741 - accuracy: 0.6051 - val_loss: 0.8454 - val_accuracy: 0.6305 - lr: 1.0000e-04\nEpoch 4/20\n186/186 [==============================] - 130s 698ms/step - loss: 0.8653 - accuracy: 0.6101 - val_loss: 0.7633 - val_accuracy: 0.6305 - lr: 1.0000e-04\nEpoch 5/20\n105/186 [===============>..............] - ETA: 53s - loss: 0.8492 - accuracy: 0.6214","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nimport os\nimport math\n\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=1e-4)  # Reduce learning rate\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef cosine_decay(epoch):\n    initial_lr = 1e-4\n    return initial_lr * (1 + math.cos(epoch * math.pi / 20)) / 2\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=2,  # Stop earlier\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:11:00.158581Z","iopub.execute_input":"2025-01-06T18:11:00.158941Z","iopub.status.idle":"2025-01-06T18:22:16.469812Z","shell.execute_reply.started":"2025-01-06T18:11:00.158912Z","shell.execute_reply":"2025-01-06T18:22:16.469080Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [00:51<00:29, 40.26it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:17<00:00, 45.14it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 43.70it/s]\nAll model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n186/186 [==============================] - 152s 732ms/step - loss: 1.5898 - accuracy: 0.3465 - val_loss: 1.2882 - val_accuracy: 0.4038 - lr: 1.0000e-04\nEpoch 2/20\n186/186 [==============================] - 131s 703ms/step - loss: 1.4807 - accuracy: 0.3926 - val_loss: 1.2170 - val_accuracy: 0.5429 - lr: 9.9384e-05\nEpoch 3/20\n186/186 [==============================] - 129s 696ms/step - loss: 1.4382 - accuracy: 0.3970 - val_loss: 1.6266 - val_accuracy: 0.2800 - lr: 9.7553e-05\nEpoch 4/20\n186/186 [==============================] - 130s 701ms/step - loss: 1.4024 - accuracy: 0.4027 - val_loss: 1.4134 - val_accuracy: 0.3581 - lr: 1.8910e-05\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 16s 467ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"This vgg19 and bert model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=0.001)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:01:08.238623Z","iopub.execute_input":"2025-01-06T17:01:08.239137Z","execution_failed":"2025-01-06T17:34:25.024Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [01:08<00:39, 29.34it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:41<00:00, 34.48it/s]\nProcessing images: 100%|██████████| 873/873 [00:24<00:00, 35.01it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n186/186 [==============================] - 170s 776ms/step - loss: 0.9227 - accuracy: 0.5835 - val_loss: 0.9093 - val_accuracy: 0.6305\nEpoch 2/20\n186/186 [==============================] - 136s 731ms/step - loss: 0.8584 - accuracy: 0.6259 - val_loss: 0.8410 - val_accuracy: 0.6305\nEpoch 3/20\n186/186 [==============================] - 135s 728ms/step - loss: 0.8521 - accuracy: 0.6293 - val_loss: 0.8337 - val_accuracy: 0.6305\nEpoch 4/20\n186/186 [==============================] - 136s 733ms/step - loss: 0.8463 - accuracy: 0.6296 - val_loss: 0.8354 - val_accuracy: 0.6305\nEpoch 5/20\n186/186 [==============================] - 136s 731ms/step - loss: 0.8430 - accuracy: 0.6300 - val_loss: 0.8362 - val_accuracy: 0.6305\nEpoch 6/20\n186/186 [==============================] - 136s 731ms/step - loss: 0.8421 - accuracy: 0.6306 - val_loss: 0.8335 - val_accuracy: 0.6305\nEpoch 7/20\n186/186 [==============================] - 136s 730ms/step - loss: 0.8431 - accuracy: 0.6306 - val_loss: 0.8365 - val_accuracy: 0.6305\nEpoch 8/20\n186/186 [==============================] - 135s 726ms/step - loss: 0.8393 - accuracy: 0.6306 - val_loss: 0.8338 - val_accuracy: 0.6305\nEpoch 9/20\n186/186 [==============================] - 136s 732ms/step - loss: 0.8448 - accuracy: 0.6306 - val_loss: 0.8363 - val_accuracy: 0.6305\nEpoch 10/20\n186/186 [==============================] - 135s 726ms/step - loss: 0.8421 - accuracy: 0.6306 - val_loss: 0.8385 - val_accuracy: 0.6305\nEpoch 11/20\n186/186 [==============================] - 135s 728ms/step - loss: 0.8391 - accuracy: 0.6306 - val_loss: 0.8359 - val_accuracy: 0.6305\nEpoch 12/20\n186/186 [==============================] - 136s 730ms/step - loss: 0.8429 - accuracy: 0.6306 - val_loss: 0.8316 - val_accuracy: 0.6305\nEpoch 13/20\n186/186 [==============================] - 136s 730ms/step - loss: 0.8398 - accuracy: 0.6306 - val_loss: 0.8342 - val_accuracy: 0.6305\nEpoch 14/20\n 28/186 [===>..........................] - ETA: 1:47 - loss: 0.8356 - accuracy: 0.6272","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:28:02.213845Z","iopub.execute_input":"2025-01-06T15:28:02.214167Z","iopub.status.idle":"2025-01-06T16:38:51.269392Z","shell.execute_reply.started":"2025-01-06T15:28:02.214142Z","shell.execute_reply":"2025-01-06T16:38:51.268536Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2326/3495 [00:51<00:29, 39.76it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:17<00:00, 45.02it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 44.10it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n186/186 [==============================] - 156s 758ms/step - loss: 0.9219 - accuracy: 0.6081 - val_loss: 0.7672 - val_accuracy: 0.7276\nEpoch 2/30\n186/186 [==============================] - 136s 734ms/step - loss: 0.8201 - accuracy: 0.6633 - val_loss: 0.7179 - val_accuracy: 0.7486\nEpoch 3/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.7634 - accuracy: 0.6936 - val_loss: 0.7111 - val_accuracy: 0.7467\nEpoch 4/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.7092 - accuracy: 0.7249 - val_loss: 0.7651 - val_accuracy: 0.7333\nEpoch 5/30\n186/186 [==============================] - 137s 737ms/step - loss: 0.6557 - accuracy: 0.7505 - val_loss: 0.7746 - val_accuracy: 0.7257\nEpoch 6/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.5581 - accuracy: 0.7882 - val_loss: 0.8606 - val_accuracy: 0.7410\nEpoch 7/30\n186/186 [==============================] - 136s 731ms/step - loss: 0.4608 - accuracy: 0.8323 - val_loss: 0.9832 - val_accuracy: 0.6876\nEpoch 8/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.3688 - accuracy: 0.8599 - val_loss: 1.0162 - val_accuracy: 0.6971\nEpoch 9/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.2733 - accuracy: 0.9037 - val_loss: 1.2261 - val_accuracy: 0.6971\nEpoch 10/30\n186/186 [==============================] - 136s 734ms/step - loss: 0.1999 - accuracy: 0.9364 - val_loss: 1.4856 - val_accuracy: 0.7086\nEpoch 11/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.1399 - accuracy: 0.9522 - val_loss: 1.6260 - val_accuracy: 0.6857\nEpoch 12/30\n186/186 [==============================] - 136s 732ms/step - loss: 0.1255 - accuracy: 0.9606 - val_loss: 1.5691 - val_accuracy: 0.7143\nEpoch 13/30\n186/186 [==============================] - 137s 737ms/step - loss: 0.0739 - accuracy: 0.9798 - val_loss: 1.7781 - val_accuracy: 0.7162\nEpoch 14/30\n186/186 [==============================] - 136s 731ms/step - loss: 0.0708 - accuracy: 0.9774 - val_loss: 2.0407 - val_accuracy: 0.6876\nEpoch 15/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.0490 - accuracy: 0.9845 - val_loss: 1.7696 - val_accuracy: 0.7029\nEpoch 16/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.0454 - accuracy: 0.9875 - val_loss: 1.9841 - val_accuracy: 0.7029\nEpoch 17/30\n186/186 [==============================] - 136s 732ms/step - loss: 0.0333 - accuracy: 0.9912 - val_loss: 2.0264 - val_accuracy: 0.7162\nEpoch 18/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.0543 - accuracy: 0.9818 - val_loss: 1.9839 - val_accuracy: 0.7219\nEpoch 19/30\n186/186 [==============================] - 137s 738ms/step - loss: 0.0320 - accuracy: 0.9902 - val_loss: 2.2057 - val_accuracy: 0.6933\nEpoch 20/30\n186/186 [==============================] - 137s 737ms/step - loss: 0.0211 - accuracy: 0.9970 - val_loss: 2.2270 - val_accuracy: 0.7181\nEpoch 21/30\n186/186 [==============================] - 137s 736ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 2.3363 - val_accuracy: 0.7162\nEpoch 22/30\n186/186 [==============================] - 136s 730ms/step - loss: 0.0197 - accuracy: 0.9953 - val_loss: 2.2267 - val_accuracy: 0.7048\nEpoch 23/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.0521 - accuracy: 0.9832 - val_loss: 2.4334 - val_accuracy: 0.6933\nEpoch 24/30\n186/186 [==============================] - 136s 734ms/step - loss: 0.0410 - accuracy: 0.9882 - val_loss: 2.2849 - val_accuracy: 0.7124\nEpoch 25/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 2.6276 - val_accuracy: 0.7010\nEpoch 26/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.0471 - accuracy: 0.9852 - val_loss: 2.1278 - val_accuracy: 0.7048\nEpoch 27/30\n186/186 [==============================] - 136s 733ms/step - loss: 0.0267 - accuracy: 0.9909 - val_loss: 2.6330 - val_accuracy: 0.7029\nEpoch 28/30\n186/186 [==============================] - 137s 735ms/step - loss: 0.0279 - accuracy: 0.9909 - val_loss: 2.5959 - val_accuracy: 0.7067\nEpoch 29/30\n186/186 [==============================] - 136s 732ms/step - loss: 0.0270 - accuracy: 0.9896 - val_loss: 2.7380 - val_accuracy: 0.7162\nEpoch 30/30\n186/186 [==============================] - 136s 734ms/step - loss: 0.0390 - accuracy: 0.9902 - val_loss: 2.8166 - val_accuracy: 0.7105\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 19s 575ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=10):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=30):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T15:11:27.045036Z","iopub.execute_input":"2025-01-06T15:11:27.045356Z","iopub.status.idle":"2025-01-06T15:27:46.780771Z","shell.execute_reply.started":"2025-01-06T15:11:27.045331Z","shell.execute_reply":"2025-01-06T15:27:46.779955Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2327/3495 [00:52<00:29, 39.91it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:18<00:00, 44.76it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 44.84it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n372/372 [==============================] - 189s 461ms/step - loss: 0.9043 - accuracy: 0.6215 - val_loss: 0.7535 - val_accuracy: 0.7295 - lr: 2.0000e-05\nEpoch 2/30\n372/372 [==============================] - 164s 442ms/step - loss: 0.8198 - accuracy: 0.6704 - val_loss: 0.7334 - val_accuracy: 0.7467 - lr: 2.0000e-05\nEpoch 3/30\n372/372 [==============================] - 163s 439ms/step - loss: 0.7379 - accuracy: 0.7067 - val_loss: 0.7349 - val_accuracy: 0.7029 - lr: 2.0000e-05\nEpoch 4/30\n372/372 [==============================] - 164s 440ms/step - loss: 0.6840 - accuracy: 0.7364 - val_loss: 0.7356 - val_accuracy: 0.7410 - lr: 2.0000e-05\nEpoch 5/30\n372/372 [==============================] - 165s 443ms/step - loss: 0.5956 - accuracy: 0.7653 - val_loss: 0.7515 - val_accuracy: 0.7429 - lr: 4.0000e-06\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 19s 571ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(512, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(512, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T10:43:23.668372Z","iopub.execute_input":"2025-01-06T10:43:23.668700Z","iopub.status.idle":"2025-01-06T11:36:22.037780Z","shell.execute_reply.started":"2025-01-06T10:43:23.668674Z","shell.execute_reply":"2025-01-06T11:36:22.036880Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [00:51<00:28, 41.07it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:16<00:00, 45.62it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 44.66it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n372/372 [==============================] - 171s 418ms/step - loss: 0.8936 - accuracy: 0.6222 - val_loss: 0.7633 - val_accuracy: 0.7505\nEpoch 2/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.8141 - accuracy: 0.6764 - val_loss: 0.7356 - val_accuracy: 0.7390\nEpoch 3/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.7576 - accuracy: 0.7037 - val_loss: 0.7010 - val_accuracy: 0.7486\nEpoch 4/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.6913 - accuracy: 0.7279 - val_loss: 0.7692 - val_accuracy: 0.7429\nEpoch 5/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.6476 - accuracy: 0.7519 - val_loss: 0.7640 - val_accuracy: 0.7486\nEpoch 6/20\n372/372 [==============================] - 152s 408ms/step - loss: 0.5789 - accuracy: 0.7710 - val_loss: 0.7597 - val_accuracy: 0.7410\nEpoch 7/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.4726 - accuracy: 0.8172 - val_loss: 0.9066 - val_accuracy: 0.7333\nEpoch 8/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.3456 - accuracy: 0.8694 - val_loss: 1.2490 - val_accuracy: 0.7238\nEpoch 9/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.2159 - accuracy: 0.9158 - val_loss: 1.5389 - val_accuracy: 0.6076\nEpoch 10/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.1494 - accuracy: 0.9481 - val_loss: 1.7067 - val_accuracy: 0.7276\nEpoch 11/20\n372/372 [==============================] - 152s 407ms/step - loss: 0.0882 - accuracy: 0.9704 - val_loss: 1.9222 - val_accuracy: 0.7276\nEpoch 12/20\n372/372 [==============================] - 152s 407ms/step - loss: 0.0628 - accuracy: 0.9805 - val_loss: 2.1038 - val_accuracy: 0.7257\nEpoch 13/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.0452 - accuracy: 0.9825 - val_loss: 2.0848 - val_accuracy: 0.7143\nEpoch 14/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.0461 - accuracy: 0.9869 - val_loss: 2.0660 - val_accuracy: 0.6933\nEpoch 15/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0317 - accuracy: 0.9906 - val_loss: 2.3325 - val_accuracy: 0.6914\nEpoch 16/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.0364 - accuracy: 0.9875 - val_loss: 2.1917 - val_accuracy: 0.7276\nEpoch 17/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0405 - accuracy: 0.9869 - val_loss: 2.3662 - val_accuracy: 0.7105\nEpoch 18/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0451 - accuracy: 0.9838 - val_loss: 2.5299 - val_accuracy: 0.6933\nEpoch 19/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 2.4237 - val_accuracy: 0.7181\nEpoch 20/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.0208 - accuracy: 0.9956 - val_loss: 2.9082 - val_accuracy: 0.7086\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 18s 522ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Less layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T11:41:58.166279Z","iopub.execute_input":"2025-01-06T11:41:58.166641Z","iopub.status.idle":"2025-01-06T12:34:48.887549Z","shell.execute_reply.started":"2025-01-06T11:41:58.166615Z","shell.execute_reply":"2025-01-06T12:34:48.886596Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2327/3495 [00:50<00:27, 42.25it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:15<00:00, 46.24it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 44.60it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n372/372 [==============================] - 171s 417ms/step - loss: 0.8941 - accuracy: 0.5980 - val_loss: 0.7894 - val_accuracy: 0.6762\nEpoch 2/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.7961 - accuracy: 0.6721 - val_loss: 0.7363 - val_accuracy: 0.7429\nEpoch 3/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.7484 - accuracy: 0.6953 - val_loss: 0.7169 - val_accuracy: 0.7467\nEpoch 4/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.7174 - accuracy: 0.7135 - val_loss: 0.6897 - val_accuracy: 0.7429\nEpoch 5/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.6920 - accuracy: 0.7253 - val_loss: 0.7425 - val_accuracy: 0.7105\nEpoch 6/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.6604 - accuracy: 0.7411 - val_loss: 0.7385 - val_accuracy: 0.7333\nEpoch 7/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.6035 - accuracy: 0.7616 - val_loss: 0.7600 - val_accuracy: 0.7448\nEpoch 8/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.5226 - accuracy: 0.8003 - val_loss: 0.8812 - val_accuracy: 0.6971\nEpoch 9/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.4499 - accuracy: 0.8226 - val_loss: 1.1163 - val_accuracy: 0.6667\nEpoch 10/20\n372/372 [==============================] - 151s 405ms/step - loss: 0.3463 - accuracy: 0.8771 - val_loss: 1.1603 - val_accuracy: 0.7314\nEpoch 11/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.2653 - accuracy: 0.9030 - val_loss: 1.4195 - val_accuracy: 0.6857\nEpoch 12/20\n372/372 [==============================] - 151s 405ms/step - loss: 0.1962 - accuracy: 0.9306 - val_loss: 1.8021 - val_accuracy: 0.7162\nEpoch 13/20\n372/372 [==============================] - 151s 407ms/step - loss: 0.1291 - accuracy: 0.9589 - val_loss: 1.7659 - val_accuracy: 0.7029\nEpoch 14/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.1024 - accuracy: 0.9704 - val_loss: 1.9538 - val_accuracy: 0.6781\nEpoch 15/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0644 - accuracy: 0.9842 - val_loss: 2.0619 - val_accuracy: 0.7067\nEpoch 16/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0534 - accuracy: 0.9855 - val_loss: 2.1485 - val_accuracy: 0.7086\nEpoch 17/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0543 - accuracy: 0.9822 - val_loss: 2.1694 - val_accuracy: 0.6914\nEpoch 18/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0461 - accuracy: 0.9875 - val_loss: 2.4430 - val_accuracy: 0.7124\nEpoch 19/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 2.4169 - val_accuracy: 0.7067\nEpoch 20/20\n372/372 [==============================] - 151s 406ms/step - loss: 0.0322 - accuracy: 0.9916 - val_loss: 2.3965 - val_accuracy: 0.6990\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 17s 518ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n        \n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n            \n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n        \n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n        \n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n        \n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n        \n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n        \n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:13:06.320848Z","iopub.execute_input":"2025-01-06T14:13:06.321144Z","iopub.status.idle":"2025-01-06T15:03:11.813150Z","shell.execute_reply.started":"2025-01-06T14:13:06.321084Z","shell.execute_reply":"2025-01-06T15:03:11.812201Z"}},"outputs":[{"name":"stderr","text":"Processing images:  66%|██████▋   | 2324/3495 [01:13<00:39, 29.63it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:49<00:00, 31.83it/s]\nProcessing images: 100%|██████████| 873/873 [00:27<00:00, 31.61it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3d6383c81947c5b6a867b155ef7985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffbb7343d4f94fafbce329fce55ea82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cbc88dbcf324e9c92cf86b52060d9f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594e4e8e3d294c3dadf85e6e6f3cd6ab"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 0s 0us/step\nEpoch 1/20\n186/186 [==============================] - 168s 767ms/step - loss: 0.9207 - accuracy: 0.5983 - val_loss: 0.7841 - val_accuracy: 0.7162\nEpoch 2/20\n186/186 [==============================] - 138s 740ms/step - loss: 0.8178 - accuracy: 0.6515 - val_loss: 0.7509 - val_accuracy: 0.7467\nEpoch 3/20\n186/186 [==============================] - 137s 734ms/step - loss: 0.7719 - accuracy: 0.6828 - val_loss: 0.7484 - val_accuracy: 0.7314\nEpoch 4/20\n186/186 [==============================] - 137s 738ms/step - loss: 0.7457 - accuracy: 0.7084 - val_loss: 0.7257 - val_accuracy: 0.7390\nEpoch 5/20\n186/186 [==============================] - 137s 738ms/step - loss: 0.6989 - accuracy: 0.7266 - val_loss: 0.7801 - val_accuracy: 0.7352\nEpoch 6/20\n186/186 [==============================] - 137s 737ms/step - loss: 0.6582 - accuracy: 0.7411 - val_loss: 0.7987 - val_accuracy: 0.6781\nEpoch 7/20\n186/186 [==============================] - 136s 734ms/step - loss: 0.5857 - accuracy: 0.7710 - val_loss: 0.8472 - val_accuracy: 0.6800\nEpoch 8/20\n186/186 [==============================] - 137s 739ms/step - loss: 0.5011 - accuracy: 0.8064 - val_loss: 0.8833 - val_accuracy: 0.7162\nEpoch 9/20\n186/186 [==============================] - 138s 741ms/step - loss: 0.4426 - accuracy: 0.8337 - val_loss: 0.9860 - val_accuracy: 0.7143\nEpoch 10/20\n186/186 [==============================] - 137s 739ms/step - loss: 0.3331 - accuracy: 0.8781 - val_loss: 1.1069 - val_accuracy: 0.7238\nEpoch 11/20\n186/186 [==============================] - 137s 739ms/step - loss: 0.2456 - accuracy: 0.9084 - val_loss: 1.3752 - val_accuracy: 0.7276\nEpoch 12/20\n186/186 [==============================] - 138s 741ms/step - loss: 0.1889 - accuracy: 0.9327 - val_loss: 1.7611 - val_accuracy: 0.6229\nEpoch 13/20\n186/186 [==============================] - 137s 736ms/step - loss: 0.1556 - accuracy: 0.9444 - val_loss: 1.8383 - val_accuracy: 0.7067\nEpoch 14/20\n186/186 [==============================] - 137s 738ms/step - loss: 0.1058 - accuracy: 0.9626 - val_loss: 1.7303 - val_accuracy: 0.7295\nEpoch 15/20\n186/186 [==============================] - 137s 735ms/step - loss: 0.0819 - accuracy: 0.9737 - val_loss: 1.7318 - val_accuracy: 0.7067\nEpoch 16/20\n186/186 [==============================] - 138s 740ms/step - loss: 0.0496 - accuracy: 0.9869 - val_loss: 1.7910 - val_accuracy: 0.7295\nEpoch 17/20\n186/186 [==============================] - 137s 736ms/step - loss: 0.0536 - accuracy: 0.9818 - val_loss: 2.0278 - val_accuracy: 0.7200\nEpoch 18/20\n186/186 [==============================] - 138s 740ms/step - loss: 0.0366 - accuracy: 0.9909 - val_loss: 1.8723 - val_accuracy: 0.7486\nEpoch 19/20\n186/186 [==============================] - 138s 740ms/step - loss: 0.0759 - accuracy: 0.9741 - val_loss: 1.8157 - val_accuracy: 0.7086\nEpoch 20/20\n186/186 [==============================] - 138s 741ms/step - loss: 0.0608 - accuracy: 0.9798 - val_loss: 1.9753 - val_accuracy: 0.7390\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 25s 649ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Edited","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nimport os\nfrom tqdm import tqdm\n\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n    def build_model(self):\n        # Image branch (VGG19)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        # Fine-tune only the top layers\n        for layer in vgg19.layers[:-4]:\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, class_weights, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs,\n        class_weights_dict\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Evaluate model\n    print(classification_report(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n    print(confusion_matrix(test_df['Label_Sentiment'].map(label_map), predicted_labels))\n\n    # Save predictions\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:41:14.208845Z","iopub.execute_input":"2025-01-06T16:41:14.209344Z","iopub.status.idle":"2025-01-06T16:52:55.961419Z","shell.execute_reply.started":"2025-01-06T16:41:14.209314Z","shell.execute_reply":"2025-01-06T16:52:55.960223Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2327/3495 [00:51<00:29, 40.17it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:17<00:00, 45.10it/s]\nProcessing images: 100%|██████████| 873/873 [00:19<00:00, 44.73it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n186/186 [==============================] - 158s 764ms/step - loss: 1.2778 - accuracy: 0.3734 - val_loss: 0.8375 - val_accuracy: 0.7200 - lr: 2.0000e-05\nEpoch 2/20\n186/186 [==============================] - 136s 731ms/step - loss: 1.1414 - accuracy: 0.4347 - val_loss: 0.7753 - val_accuracy: 0.7086 - lr: 2.0000e-05\nEpoch 3/20\n186/186 [==============================] - 136s 734ms/step - loss: 1.0411 - accuracy: 0.4727 - val_loss: 0.9039 - val_accuracy: 0.5962 - lr: 2.0000e-05\nEpoch 4/20\n186/186 [==============================] - 137s 737ms/step - loss: 0.9934 - accuracy: 0.4923 - val_loss: 0.8884 - val_accuracy: 0.6495 - lr: 2.0000e-05\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 19s 571ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Label_Sentiment'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3051851506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/3051851506.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Label_Sentiment'"],"ename":"KeyError","evalue":"'Label_Sentiment'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\nimport tensorflow_addons as tfa\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Focal loss for better handling of class imbalance\n    loss = tfa.losses.SigmoidFocalCrossEntropy()\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss=loss,\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:38:09.070207Z","iopub.execute_input":"2025-01-06T18:38:09.070634Z","iopub.status.idle":"2025-01-06T18:41:26.481732Z","shell.execute_reply.started":"2025-01-06T18:38:09.070548Z","shell.execute_reply":"2025-01-06T18:41:26.480188Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [01:13<00:41, 28.19it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:49<00:00, 31.90it/s]\nProcessing images: 100%|██████████| 873/873 [00:27<00:00, 31.90it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbc0b16060d3409b8fcbf837623e03fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1b2847316df4828afbef784e3ca82c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"995c21855d7942678621a869a8ba9d26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae101b8f6f404ae8a3fa27fecf893857"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 2s 0us/step\nEpoch 1/20\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3236994921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/3236994921.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m     model, history = train_model(\n\u001b[1;32m    229\u001b[0m         \u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mval_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3236994921.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 238, in __call__\n        total_loss_metric_value = tf.add_n(loss_metric_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN_1}} = AddN[N=2, T=DT_FLOAT](sigmoid_focal_crossentropy/weighted_loss/Mul, AddN)' with input shapes: [?], [].\n"],"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 949, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 238, in __call__\n        total_loss_metric_value = tf.add_n(loss_metric_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN_1}} = AddN[N=2, T=DT_FLOAT](sigmoid_focal_crossentropy/weighted_loss/Mul, AddN)' with input shapes: [?], [].\n","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"Final","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Concatenate, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport os\nimport math\n\n# Load and preprocess data\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.vgg19.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n\ndef augment_images(images):\n    \"\"\"Apply stronger data augmentation\"\"\"\n    datagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        shear_range=0.3,\n        zoom_range=0.3,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        brightness_range=[0.8, 1.2]\n    )\n    return datagen.flow(images, batch_size=16, shuffle=True)\n\n\n# Multimodal Model Definition\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        # Use XLM-Roberta for multilingual capabilities\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n        self.bert_model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n\n    def build_model(self):\n        # Image branch (VGG19 with fine-tuning)\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        vgg19 = VGG19(weights='imagenet', include_top=False)\n\n        for layer in vgg19.layers[:-8]:  # Fine-tune deeper layers\n            layer.trainable = False\n\n        x = vgg19(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n        x = Dropout(0.4)(x)\n        image_features = BatchNormalization()(x)\n\n        # Text branch (XLM-Roberta)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])\n        pooled_output = bert_outputs[1]  # Use the pooled output\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = BatchNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(combined)\n        x = Dropout(0.3)(x)\n        x = BatchNormalization()(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        return model\n\n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using XLM-Roberta tokenizer\"\"\"\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\n\n# Cosine Annealing Learning Rate Scheduler\ndef lr_schedule(epoch):\n    initial_lr = 1e-4\n    if epoch < 5:  # Warm-up phase\n        return initial_lr * (epoch + 1) / 5\n    else:\n        return initial_lr * tf.math.exp(-0.1 * (epoch - 5))\n\n\n# Model Training\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n\n    # Convert labels to one-hot encoding\n    train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n    val_labels_onehot = tf.keras.utils.to_categorical(val_labels, num_classes=3)\n\n    # Compute class weights for imbalanced datasets\n    class_weights = class_weight.compute_class_weight(\n        class_weight='balanced',\n        classes=np.unique(train_labels),\n        y=train_labels\n    )\n    class_weights = dict(enumerate(class_weights))\n\n    # Compile the model\n    model.compile(\n        optimizer=Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        lr_callback\n    ]\n\n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels_onehot,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels_onehot\n        ),\n        class_weight=class_weights,\n        epochs=epochs,\n        batch_size=32,\n        callbacks=callbacks\n    )\n\n    return model, history\n\n\n# Main Function\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n\n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n\n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n\n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n\n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n\n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n\n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n\n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n\n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n\n    return model, history\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T18:47:32.681831Z","iopub.execute_input":"2025-01-06T18:47:32.682182Z","iopub.status.idle":"2025-01-06T18:59:22.905496Z","shell.execute_reply.started":"2025-01-06T18:47:32.682107Z","shell.execute_reply":"2025-01-06T18:59:22.904502Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2326/3495 [01:11<00:39, 29.51it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:46<00:00, 32.67it/s]\nProcessing images: 100%|██████████| 873/873 [00:26<00:00, 32.99it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b26184c1c174cbea9168119abeb03f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bc03a58abe47daac37e274e83c5408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc816f54cd54e4f9f9845c28a6f9523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64897eb108884c89817ec8b6d1eb0d9c"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 0s 0us/step\nEpoch 1/20\n93/93 [==============================] - 148s 1s/step - loss: 1.5550 - accuracy: 0.3492 - val_loss: 1.2519 - val_accuracy: 0.6286 - lr: 2.0000e-05\nEpoch 2/20\n93/93 [==============================] - 113s 1s/step - loss: 1.6000 - accuracy: 0.3586 - val_loss: 1.0532 - val_accuracy: 0.5219 - lr: 4.0000e-05\nEpoch 3/20\n93/93 [==============================] - 113s 1s/step - loss: 1.6244 - accuracy: 0.3431 - val_loss: 1.1412 - val_accuracy: 0.4457 - lr: 6.0000e-05\nEpoch 4/20\n93/93 [==============================] - 114s 1s/step - loss: 1.5085 - accuracy: 0.3774 - val_loss: 1.2362 - val_accuracy: 0.2990 - lr: 8.0000e-05\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 17s 489ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T05:53:13.106974Z","iopub.execute_input":"2025-01-07T05:53:13.107721Z","iopub.status.idle":"2025-01-07T05:53:32.898832Z","shell.execute_reply.started":"2025-01-07T05:53:13.107689Z","shell.execute_reply":"2025-01-07T05:53:32.897700Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.9.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting vit-keras\n  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (9.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.48.1)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.27.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.2.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from vit-keras) (1.7.3)\nCollecting validators\n  Downloading validators-0.20.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from validators->vit-keras) (5.1.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\nBuilding wheels for collected packages: validators\n  Building wheel for validators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=0e80d0aa3d43caace847c9389f963294aad00996ec91addd3a2974edc0e1ff6d\n  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\nSuccessfully built validators\nInstalling collected packages: validators, protobuf, vit-keras\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.10.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.61.0 which is incompatible.\ntfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.9.2 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.19.6 which is incompatible.\nonnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nnnabla 1.34.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-logging 3.2.2 requires google-cloud-core<3.0.0dev,>=2.0.0, but you have google-cloud-core 1.7.3 which is incompatible.\ngoogle-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\ngcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\napache-beam 2.41.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.41.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.19.6 validators-0.20.0 vit-keras-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu')(x)  # Add Dense layer instead of pooling\n        x = Dropout(0.3)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu')(pooled_output)\n        x = Dropout(0.3)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu')(combined)\n        x = Dropout(0.3)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_accuracy',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        )\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=16,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T06:13:41.793368Z","iopub.execute_input":"2025-01-07T06:13:41.793810Z","iopub.status.idle":"2025-01-07T06:45:12.973797Z","shell.execute_reply.started":"2025-01-07T06:13:41.793732Z","shell.execute_reply":"2025-01-07T06:45:12.972784Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [01:01<00:34, 33.75it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:30<00:00, 38.45it/s]\nProcessing images: 100%|██████████| 873/873 [00:22<00:00, 38.73it/s]\nSome layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n/opt/conda/lib/python3.7/site-packages/vit_keras/utils.py:83: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n186/186 [==============================] - 280s 1s/step - loss: 0.9405 - accuracy: 0.5741 - val_loss: 0.9045 - val_accuracy: 0.6324 - lr: 2.0000e-05\nEpoch 2/20\n186/186 [==============================] - 238s 1s/step - loss: 0.8600 - accuracy: 0.6182 - val_loss: 0.7655 - val_accuracy: 0.7067 - lr: 2.0000e-05\nEpoch 3/20\n186/186 [==============================] - 237s 1s/step - loss: 0.8195 - accuracy: 0.6522 - val_loss: 0.7405 - val_accuracy: 0.6933 - lr: 2.0000e-05\nEpoch 4/20\n186/186 [==============================] - 238s 1s/step - loss: 0.7555 - accuracy: 0.6822 - val_loss: 0.7692 - val_accuracy: 0.7105 - lr: 2.0000e-05\nEpoch 5/20\n186/186 [==============================] - 237s 1s/step - loss: 0.6919 - accuracy: 0.7172 - val_loss: 0.7466 - val_accuracy: 0.7029 - lr: 2.0000e-05\nEpoch 6/20\n186/186 [==============================] - 238s 1s/step - loss: 0.5794 - accuracy: 0.7896 - val_loss: 0.8993 - val_accuracy: 0.6990 - lr: 4.0000e-06\nEpoch 7/20\n186/186 [==============================] - 237s 1s/step - loss: 0.4824 - accuracy: 0.8290 - val_loss: 1.0519 - val_accuracy: 0.6838 - lr: 4.0000e-06\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"28/28 [==============================] - 27s 763ms/step\nPredictions saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install tensorflow numpy pandas transformers vit-keras tqdm scikit-learn pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:33:48.161190Z","iopub.execute_input":"2025-01-07T08:33:48.161544Z","iopub.status.idle":"2025-01-07T08:34:09.481692Z","shell.execute_reply.started":"2025-01-07T08:33:48.161516Z","shell.execute_reply":"2025-01-07T08:34:09.480417Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.9.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting vit-keras\n  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (9.2.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.27.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.1)\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.48.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.2.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from vit-keras) (1.7.3)\nCollecting validators\n  Downloading validators-0.20.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from validators->vit-keras) (5.1.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\nBuilding wheels for collected packages: validators\n  Building wheel for validators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=d15fae45053b4072f048f1a3b59eb610e22a5b2924fa8c862dcb5b0321fdffb3\n  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\nSuccessfully built validators\nInstalling collected packages: validators, protobuf, vit-keras\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.10.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.61.0 which is incompatible.\ntfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.9.2 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.19.6 which is incompatible.\nonnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nnnabla 1.34.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-logging 3.2.2 requires google-cloud-core<3.0.0dev,>=2.0.0, but you have google-cloud-core 1.7.3 which is incompatible.\ngoogle-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\ngcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\napache-beam 2.41.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.41.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.19.6 validators-0.20.0 vit-keras-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',  # Monitor validation loss\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,  # Decreased batch size for better generalization\n        class_weight=class_weight_dict,  # Add class weights\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T08:34:31.838915Z","iopub.execute_input":"2025-01-07T08:34:31.839388Z","execution_failed":"2025-01-07T09:14:48.518Z"}},"outputs":[{"name":"stderr","text":"Processing images:  66%|██████▋   | 2324/3495 [01:15<00:39, 29.84it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:56<00:00, 30.07it/s]\nProcessing images: 100%|██████████| 873/873 [00:30<00:00, 28.60it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a35f75366f1451db514d93985187bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbb2cfdcf134c52b052a6c1ef94ec1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972f7425f43340c2a138c84a17fdfd98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d385c1c520cb4a4185a27c1ec2d1fa91"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n347502902/347502902 [==============================] - 10s 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/vit_keras/utils.py:83: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n372/372 [==============================] - 308s 734ms/step - loss: 14.0193 - accuracy: 0.3300 - val_loss: 13.3902 - val_accuracy: 0.5848 - lr: 2.0000e-05\nEpoch 2/20\n372/372 [==============================] - 269s 722ms/step - loss: 13.5225 - accuracy: 0.3562 - val_loss: 13.0450 - val_accuracy: 0.6229 - lr: 2.0000e-05\nEpoch 3/20\n372/372 [==============================] - 269s 722ms/step - loss: 13.1062 - accuracy: 0.3508 - val_loss: 12.7356 - val_accuracy: 0.4210 - lr: 2.0000e-05\nEpoch 4/20\n372/372 [==============================] - 268s 721ms/step - loss: 12.6572 - accuracy: 0.3566 - val_loss: 12.2638 - val_accuracy: 0.5790 - lr: 2.0000e-05\nEpoch 5/20\n372/372 [==============================] - 268s 720ms/step - loss: 12.1996 - accuracy: 0.3791 - val_loss: 11.7907 - val_accuracy: 0.6229 - lr: 2.0000e-05\nEpoch 6/20\n372/372 [==============================] - 268s 721ms/step - loss: 11.7251 - accuracy: 0.4205 - val_loss: 11.3743 - val_accuracy: 0.5676 - lr: 2.0000e-05\nEpoch 7/20\n372/372 [==============================] - 268s 720ms/step - loss: 11.2574 - accuracy: 0.4178 - val_loss: 11.0297 - val_accuracy: 0.2800 - lr: 2.0000e-05\nEpoch 8/20\n372/372 [==============================] - 269s 722ms/step - loss: 10.7919 - accuracy: 0.4300 - val_loss: 10.5840 - val_accuracy: 0.2895 - lr: 2.0000e-05\nEpoch 9/20\n  9/372 [..............................] - ETA: 4:06 - loss: 10.5443 - accuracy: 0.3333","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow pandas numpy tqdm scikit-learn transformers vit-keras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:17:53.255584Z","iopub.execute_input":"2025-01-07T09:17:53.255924Z","iopub.status.idle":"2025-01-07T09:18:13.372949Z","shell.execute_reply.started":"2025-01-07T09:17:53.255900Z","shell.execute_reply":"2025-01-07T09:18:13.371612Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.9.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting vit-keras\n  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.48.1)\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.27.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.2.1)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nCollecting validators\n  Downloading validators-0.20.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from validators->vit-keras) (5.1.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\nBuilding wheels for collected packages: validators\n  Building wheel for validators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=c30035cff4317efce7adba940830654d609316bd1be3d897d53bee2033a0eb6b\n  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\nSuccessfully built validators\nInstalling collected packages: validators, protobuf, vit-keras\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.10.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.61.0 which is incompatible.\ntfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.9.2 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.19.6 which is incompatible.\nonnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nnnabla 1.34.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-logging 3.2.2 requires google-cloud-core<3.0.0dev,>=2.0.0, but you have google-cloud-core 1.7.3 which is incompatible.\ngoogle-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\ngcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\napache-beam 2.41.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.41.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.19.6 validators-0.20.0 vit-keras-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom vit_keras import vit\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = tf.keras.applications.imagenet_utils.preprocess_input(img)\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\n# Image augmentation\ndef augment_images(images):\n    datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    return datagen.flow(images, batch_size=len(images), shuffle=False).next()\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (Vision Transformer)\n        vit_model = vit.vit_b16(\n            image_size=224,\n            pretrained=True,\n            include_top=False,\n            pretrained_top=False\n        )\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vit_model(image_input)  # Output: (None, 768)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # Add Dense layer with L2 regularization\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Augment training images\n    train_images = augment_images(train_images)\n\n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        class_weight=class_weight_dict,\n        callbacks=callbacks\n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:18:28.830186Z","iopub.execute_input":"2025-01-07T09:18:28.830493Z","execution_failed":"2025-01-07T09:36:14.932Z"}},"outputs":[{"name":"stderr","text":"Processing images:  66%|██████▋   | 2324/3495 [01:07<00:35, 32.64it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:41<00:00, 34.46it/s]\nProcessing images: 100%|██████████| 873/873 [00:25<00:00, 34.03it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8885917376fe49ac9787934be6b59d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6956c1b0d44acba0a41381a7e485df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ff3ee5339140708dd513cfc7bdce83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfe7f1de466491682a4c7d4598f1f75"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n347502902/347502902 [==============================] - 1s 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/vit_keras/utils.py:83: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n372/372 [==============================] - 317s 759ms/step - loss: 13.9317 - accuracy: 0.3687 - val_loss: 13.6262 - val_accuracy: 0.1867 - lr: 2.0000e-05\nEpoch 2/20\n372/372 [==============================] - 272s 733ms/step - loss: 13.4019 - accuracy: 0.3862 - val_loss: 13.3275 - val_accuracy: 0.2229 - lr: 2.0000e-05\nEpoch 3/20\n372/372 [==============================] - 272s 733ms/step - loss: 12.9636 - accuracy: 0.3795 - val_loss: 12.7777 - val_accuracy: 0.1524 - lr: 2.0000e-05\nEpoch 4/20\n  8/372 [..............................] - ETA: 4:12 - loss: 12.8576 - accuracy: 0.2656","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pip install tensorflow pandas numpy tqdm scikit-learn transformers vit-keras langdetect googletrans==4.0.0-rc1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:42:30.519888Z","iopub.execute_input":"2025-01-07T09:42:30.520750Z","iopub.status.idle":"2025-01-07T09:42:55.691280Z","shell.execute_reply.started":"2025-01-07T09:42:30.520706Z","shell.execute_reply":"2025-01-07T09:42:55.690107Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.9.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nCollecting vit-keras\n  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting httpx==0.13.3\n  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpcore==0.9.*\n  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting chardet==3.*\n  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting hstspreload\n  Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rfc3986<2,>=1.3\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2022.12.7)\nCollecting idna==2.*\n  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\nCollecting h11<0.10,>=0.8\n  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting h2==3.*\n  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting hpack<4,>=3.0\n  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nCollecting hyperframe<6,>=5.2.0\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.2.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.1.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.27.0)\nRequirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.1)\nRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.6)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\nRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.48.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.2.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\nCollecting validators\n  Downloading validators-0.20.0.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\nRequirement already satisfied: decorator>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from validators->vit-keras) (5.1.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.1)\nBuilding wheels for collected packages: googletrans, langdetect, validators\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17416 sha256=a9645a338aba104516882d897d815d5fc638ae1fd389c56d9a31211758c71895\n  Stored in directory: /root/.cache/pip/wheels/43/34/00/4fe71786ea6d12314b29037620c36d857e5d104ac2748bf82a\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=16694c6008dfe6c477376b4f4ad122a006482d448b087d9b2f6ebf29b793c1f5\n  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n  Building wheel for validators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=854b131cec62ce1e6a2a3bb611938354b2bb11b240ed18922b9ec4491e19546e\n  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\nSuccessfully built googletrans langdetect validators\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, validators, protobuf, langdetect, idna, hstspreload, h2, vit-keras, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: idna\n    Found existing installation: idna 3.3\n    Uninstalling idna-3.3:\n      Successfully uninstalled idna-3.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.10.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.61.0 which is incompatible.\ntfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\ntensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.9.2 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.2 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.19.6 which is incompatible.\nonnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nnnabla 1.34.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\ngoogle-cloud-logging 3.2.2 requires google-cloud-core<3.0.0dev,>=2.0.0, but you have google-cloud-core 1.7.3 which is incompatible.\ngoogle-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\ngcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\napache-beam 2.41.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.41.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 langdetect-1.0.9 protobuf-3.19.6 rfc3986-1.5.0 validators-0.20.0 vit-keras-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Concatenate, LayerNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom transformers import TFBertModel, BertTokenizer\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\nfrom langdetect import detect\nfrom googletrans import Translator\n\ntranslator = Translator()\n\n# Bengali stopwords list (manually curated or from libraries like bnltk)\nBENGALI_STOPWORDS = set([\"এবং\", \"কিন্তু\", \"যদি\", \"তবে\", \"অতএব\", \"অথচ\", \"যেমন\", \"তেমন\", \"কেন\", \"কখন\", \"যা\", \"তাহলে\"])\n\n# Function to detect language and transliterate Banglish to Bengali\ndef preprocess_text(text):\n    try:\n        lang = detect(text)\n        if lang == \"bn\":  # Bengali\n            text = text\n        elif lang == \"en\":  # English\n            text = text.lower()  # Convert to lowercase\n        else:  # Banglish or other languages\n            text = translator.translate(text, src=\"en\", dest=\"bn\").text\n\n        # Remove special characters and punctuation\n        text = ''.join(e for e in text if e.isalnum() or e.isspace())\n        # Remove stopwords\n        text = ' '.join(word for word in text.split() if word not in BENGALI_STOPWORDS)\n    except Exception as e:\n        print(f\"Error processing text: {text}, {e}\")\n        return \"\"\n    return text\n\ndef load_data(train_path, test_path):\n    \"\"\"Load training and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n\n    # Preprocess captions\n    train_df['Captions'] = train_df['Captions'].apply(preprocess_text)\n    test_df['Captions'] = test_df['Captions'].apply(preprocess_text)\n\n    return train_df, test_df\n\ndef get_image_paths(directory, image_names):\n    \"\"\"Get full paths for images\"\"\"\n    image_paths = {img: os.path.join(directory, img) for img in image_names}\n    return [image_paths[img] for img in image_names if img in image_paths]\n\ndef preprocess_image(image_path, target_size=(224, 224)):\n    \"\"\"Load and preprocess a single image\"\"\"\n    try:\n        img = load_img(image_path, target_size=target_size)\n        img = img_to_array(img)\n        img = vgg_preprocess(img)  # VGG19 preprocessing\n        return img\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {str(e)}\")\n        return np.zeros(target_size + (3,))\n\ndef process_images(image_paths, target_size=(224, 224)):\n    \"\"\"Process all images with progress bar\"\"\"\n    images = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        img = preprocess_image(path, target_size)\n        images.append(img)\n    return np.array(images)\n\nclass MultimodalSentimentModel:\n    def __init__(self, num_classes=3, max_length=128):\n        self.num_classes = num_classes\n        self.max_length = max_length\n        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        \n    def build_model(self):\n        # Image branch (VGG19)\n        vgg19_base = VGG19(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n        for layer in vgg19_base.layers:\n            layer.trainable = False  # Freeze VGG19 layers\n\n        image_input = Input(shape=(224, 224, 3), name='image_input')\n        x = vgg19_base(image_input)\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.4)(x)\n        image_features = LayerNormalization()(x)\n\n        # Text branch (BERT)\n        input_ids = Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask')\n\n        bert_outputs = self.bert_model([input_ids, attention_mask])[0]\n        pooled_output = tf.reduce_mean(bert_outputs, axis=1)\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(pooled_output)\n        x = Dropout(0.4)(x)\n        text_features = LayerNormalization()(x)\n\n        # Combine features\n        combined = Concatenate()([image_features, text_features])\n        x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n        x = Dropout(0.4)(x)\n        x = LayerNormalization()(x)\n        x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n        x = Dropout(0.3)(x)\n        outputs = Dense(self.num_classes, activation='softmax')(x)\n\n        model = Model(\n            inputs=[image_input, input_ids, attention_mask],\n            outputs=outputs\n        )\n\n        optimizer = Adam(learning_rate=2e-5)\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        return model\n    \n    def prepare_text(self, texts):\n        \"\"\"Tokenize texts using BERT tokenizer\"\"\"\n        encodings = self.bert_tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='tf'\n        )\n        return encodings['input_ids'], encodings['attention_mask']\n\ndef train_model(train_images, train_texts, train_labels, val_images, val_texts, val_labels, epochs=20):\n    # Create model instance\n    model_handler = MultimodalSentimentModel()\n    model = model_handler.build_model()\n    \n    # Prepare text data\n    train_input_ids, train_attention_mask = model_handler.prepare_text(train_texts)\n    val_input_ids, val_attention_mask = model_handler.prepare_text(val_texts)\n    \n    # Compute class weights\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weight_dict = dict(enumerate(class_weights))\n\n    # Callbacks\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=2,\n            min_lr=1e-6\n        ),\n        tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 if epoch > 10 else 2e-5)\n    ]\n    \n    # Train model\n    history = model.fit(\n        {\n            'image_input': train_images,\n            'input_ids': train_input_ids,\n            'attention_mask': train_attention_mask\n        },\n        train_labels,\n        validation_data=(\n            {\n                'image_input': val_images,\n                'input_ids': val_input_ids,\n                'attention_mask': val_attention_mask\n            },\n            val_labels\n        ),\n        epochs=epochs,\n        batch_size=8,\n        class_weight=class_weight_dict,\n        \n    )\n    \n    return model, history\n\ndef main():\n    # Load data\n    train_df, test_df = load_data(\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/train.csv',\n        '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/test.csv'\n    )\n    \n    # Get image paths\n    memes_folder = '/kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes'\n    train_image_paths = get_image_paths(memes_folder, train_df['image_name'].tolist())\n    test_image_paths = get_image_paths(memes_folder, test_df['image_name'].tolist())\n    \n    # Process images\n    train_images = process_images(train_image_paths)\n    test_images = process_images(test_image_paths)\n    \n    # Convert labels\n    label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n    train_labels = np.array([label_map[label] for label in train_df['Label_Sentiment']])\n    \n    # Split data\n    train_imgs, val_imgs, train_texts, val_texts, train_labs, val_labs = train_test_split(\n        train_images, train_df['Captions'],\n        train_labels, test_size=0.15,\n        random_state=42, stratify=train_labels\n    )\n    \n    # Train model\n    model, history = train_model(\n        train_imgs, train_texts, train_labs,\n        val_imgs, val_texts, val_labs\n    )\n    \n    # Prepare test data\n    model_handler = MultimodalSentimentModel()\n    test_input_ids, test_attention_mask = model_handler.prepare_text(test_df['Captions'])\n    \n    # Make predictions\n    predictions = model.predict({\n        'image_input': test_images,\n        'input_ids': test_input_ids,\n        'attention_mask': test_attention_mask\n    })\n    predicted_labels = np.argmax(predictions, axis=1)\n    # Convert predictions to labels\n    reverse_label_map = {v: k for k, v in label_map.items()}\n    test_df['Label'] = [reverse_label_map[label] for label in predicted_labels]\n    \n    # Save predictions\n    test_df[['Id', 'Label']].to_csv('submission.csv', index=False)\n    print(\"Predictions saved to submission.csv\")\n    \n    return model, history\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T09:44:39.725628Z","iopub.execute_input":"2025-01-07T09:44:39.725956Z"}},"outputs":[{"name":"stderr","text":"Processing images:  67%|██████▋   | 2325/3495 [01:05<00:37, 31.06it/s]","output_type":"stream"},{"name":"stdout","text":"Error processing image /kaggle/input/multimodal-sentiment-analysis-cuet-nlp/Memes/Memes/nurani-memes (149).jpg: image file is truncated (3 bytes not processed)\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 3495/3495 [01:38<00:00, 35.53it/s]\nProcessing images: 100%|██████████| 873/873 [00:25<00:00, 34.67it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297aeae3cb7942048a41f2ce135e216b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90ec12ba6444418a54408007eddec56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533bcca87730422b8d65160b36e0c724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d127bd104b4d45a2e70eec6303ba02"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 4s 0us/step\nEpoch 1/20\n372/372 [==============================] - 162s 381ms/step - loss: 13.4982 - accuracy: 0.3761 - val_loss: 13.2048 - val_accuracy: 0.2019\nEpoch 2/20\n372/372 [==============================] - 135s 364ms/step - loss: 13.0011 - accuracy: 0.3492 - val_loss: 12.7347 - val_accuracy: 0.2438\nEpoch 3/20\n372/372 [==============================] - 135s 363ms/step - loss: 12.6000 - accuracy: 0.3667 - val_loss: 12.5455 - val_accuracy: 0.0895\nEpoch 4/20\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"SCHOLAR","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom transformers import BertModel, BertTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Image Preprocessing and Augmentation\nimage_transforms = {\n    \"train\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Standard normalization for ImageNet\n    ]),\n    \"val\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Define Dataset\nclass MultimodalDataset(Dataset):\n    def __init__(self, image_paths, text_data, labels, transform=None, tokenizer=None):\n        self.image_paths = image_paths\n        self.text_data = text_data\n        self.labels = labels\n        self.transform = transform\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Load and preprocess image\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize text\n        text = self.text_data[idx]\n        encoded_text = self.tokenizer(\n            text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n        )\n\n        # Extract inputs and attention mask\n        input_ids = encoded_text[\"input_ids\"].squeeze(0)\n        attention_mask = encoded_text[\"attention_mask\"].squeeze(0)\n\n        label = torch.tensor(self.labels[idx])\n        return image, input_ids, attention_mask, label\n\n# Define Image Model (ResNet)\nclass ImageModel(nn.Module):\n    def __init__(self, output_dim):\n        super(ImageModel, self).__init__()\n        resnet = models.resnet18(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n        self.fc = nn.Linear(resnet.fc.in_features, output_dim)\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        features = features.view(features.size(0), -1)\n        return self.fc(features)\n\n# Define Text Model (BERT)\nclass TextModel(nn.Module):\n    def __init__(self, output_dim):\n        super(TextModel, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_token_output = outputs.last_hidden_state[:, 0, :]\n        return self.fc(cls_token_output)\n\n# Define Combined Model\nclass MultimodalModel(nn.Module):\n    def __init__(self, img_output_dim, text_output_dim, final_output_dim):\n        super(MultimodalModel, self).__init__()\n        self.image_model = ImageModel(img_output_dim)\n        self.text_model = TextModel(text_output_dim)\n        self.fc = nn.Linear(img_output_dim + text_output_dim, final_output_dim)\n\n    def forward(self, image, input_ids, attention_mask):\n        img_features = self.image_model(image)\n        text_features = self.text_model(input_ids, attention_mask)\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        return self.fc(combined_features)\n\n# Training Function\ndef train(model, dataloaders, criterion, optimizer, scheduler, device, epochs=10):\n    best_acc = 0.0\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            preds, targets = [], []\n\n            for images, input_ids, attention_mask, labels in dataloaders[phase]:\n                images = images.to(device)\n                input_ids = input_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # Forward pass\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = model(images, input_ids, attention_mask)\n                    loss = criterion(outputs, labels)\n                    _, predictions = torch.max(outputs, 1)\n\n                    if phase == \"train\":\n                        loss.backward()\n                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n                        optimizer.step()\n\n                # Track metrics\n                running_loss += loss.item() * images.size(0)\n                preds.extend(predictions.cpu().numpy())\n                targets.extend(labels.cpu().numpy())\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = accuracy_score(targets, preds)\n            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n\n            # Track the best model\n            if phase == \"val\" and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                torch.save(model.state_dict(), \"best_model.pth\")\n        scheduler.step()\n\n    print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n\n# Evaluation Function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    preds, targets = [], []\n\n    with torch.no_grad():\n        for images, input_ids, attention_mask, labels in dataloader:\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            _, predictions = torch.max(outputs, 1)\n            preds.extend(predictions.cpu().numpy())\n            targets.extend(labels.cpu().numpy())\n\n    # Compute Metrics\n    acc = accuracy_score(targets, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average=\"weighted\")\n    print(f\"Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n\n# Main Function\nif __name__ == \"__main__\":\n    # Load dataset (replace with your dataset)\n    from PIL import Image\n    import os\n\n    # Example: Replace with actual data\n    image_paths = [\"path_to_images/image1.jpg\", \"path_to_images/image2.jpg\"]  # Add your image paths\n    text_data = [\"Sample text 1\", \"Sample text 2\"]  # Add your text data\n    labels = [0, 1]  # Add your labels\n\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    dataset = {\n        \"train\": MultimodalDataset(image_paths, text_data, labels, image_transforms[\"train\"], tokenizer),\n        \"val\": MultimodalDataset(image_paths, text_data, labels, image_transforms[\"val\"], tokenizer),\n    }\n    dataloaders = {\n        phase: DataLoader(dataset[phase], batch_size=4, shuffle=(phase == \"train\")) for phase in [\"train\", \"val\"]\n    }\n\n    # Initialize model, loss, optimizer, and scheduler\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = MultimodalModel(img_output_dim=128, text_output_dim=128, final_output_dim=2).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n    # Train and Evaluate\n    train(model, dataloaders, criterion, optimizer, scheduler, device, epochs=10)\n    evaluate(model, dataloaders[\"val\"], device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T18:48:53.819378Z","iopub.execute_input":"2025-01-07T18:48:53.819637Z","iopub.status.idle":"2025-01-07T18:49:06.001369Z","shell.execute_reply.started":"2025-01-07T18:48:53.819615Z","shell.execute_reply":"2025-01-07T18:49:06.000085Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, transforms\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel, BertTokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"],"ename":"ModuleNotFoundError","evalue":"No module named 'transformers'","output_type":"error"}],"execution_count":1}]}